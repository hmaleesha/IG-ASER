Speech Emotion Recognition (SER) uses voice features to identify emotions, while Automatic SER (ASER) uses Machine Learning (ML) methods to identify human emotions automatically. ASER interpretability is crucial for understanding the relationship between acoustic features and emotional states, providing insights into ML decisions. Deep learning embeddings in ASER often lack clear interpretability due to their high-dimensional and abstract nature. This study introduces a post-hoc interpretability method for ASER features based on deep learning features' contribution to emotion predictions. The experimental ASER model achieved a validation accuracy of 77\% on the IEMOCAP dataset. The Integrated Gradient (IG) method calculates attribution values for the current model's features, which are evaluated using the Insertion/Deletion metric. The proposed method analyzes the relationship between ASER features and predictions by providing understandable interpretations based on IG values.
