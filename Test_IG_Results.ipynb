{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb91a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the checkpoint\n",
    "model = torch.load('models/best_six.pth.tar')\n",
    "\n",
    "# Check the contents of the checkpoint\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cba8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "def downsample_with_max_pooling(array, factor=(1, 4)):\n",
    "    if np.all(np.array(factor, int) == 1):\n",
    "        return array\n",
    "\n",
    "    sections = []\n",
    "\n",
    "    for offset in np.ndindex(factor):\n",
    "        part = array[tuple(np.s_[o::f] for o, f in zip(offset, factor))]\n",
    "        sections.append(part)\n",
    "\n",
    "    output = sections[0].copy()\n",
    "\n",
    "    for section in sections[1:]:\n",
    "        if output.shape == section.shape:\n",
    "            np.maximum(output, section, output)\n",
    "        else:\n",
    "            if output.shape[0] != section.shape[0]:\n",
    "                c = output.shape[0] - section.shape[0]\n",
    "                pad = np.zeros((c, output.shape[1]))\n",
    "                s = np.vstack((section, pad))\n",
    "                np.maximum(output, s, output)\n",
    "            if output.shape[1] != section.shape[1]:\n",
    "                c = output.shape[1] - section.shape[1]\n",
    "                pad = np.zeros((output.shape[0], c))\n",
    "                s = np.hstack((section, pad))\n",
    "                np.maximum(output, s, output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f594fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "  \n",
    "def pitch_and_intensity_from_audio(path, sr, duration):\n",
    "    wav, sr = librosa.load(path, sr=sr)\n",
    "    \n",
    "    audio_segment = split_audio(wav, sr, duration)\n",
    "\n",
    "    f0 = librosa.yin(audio_segment[0], fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'), sr=sr)\n",
    "    # Extract intensity using Root Mean Square (RMS)\n",
    "\n",
    "    intensity = librosa.feature.rms(y=audio_segment[0], frame_length=1024, hop_length=512)\n",
    "\n",
    "    f0_downsampled = downsample_with_max_pooling(f0.reshape(1, -1), (1, 4))\n",
    "    intensity_downsampled = downsample_with_max_pooling(intensity, (1, 4))\n",
    "    \n",
    "    return f0_downsampled, intensity_downsampled\n",
    "\n",
    "\n",
    "def add_missing_padding(audio, sr, duration):\n",
    "    signal_length = duration * sr\n",
    "    audio_length = audio.shape[0]\n",
    "    padding_length = signal_length - audio_length\n",
    "    if padding_length > 0:\n",
    "        padding = np.zeros(padding_length)\n",
    "        signal = np.hstack((audio, padding))\n",
    "        return signal\n",
    "    return audio\n",
    "\n",
    "\n",
    "def split_audio(signal, sr, split_duration):\n",
    "    length = split_duration * sr\n",
    "\n",
    "    if length < len(signal):\n",
    "        frames = librosa.util.frame(signal, frame_length=length, hop_length=length).T\n",
    "        return frames\n",
    "    else:\n",
    "        audio = add_missing_padding(signal, sr, split_duration)\n",
    "        frames = [audio]\n",
    "        return np.array(frames)\n",
    "\n",
    "def mfcc_from_audio_file(path, sr, duration):\n",
    "    wav, sr = librosa.load(path, sr=sr)\n",
    "\n",
    "    audio_segment = split_audio(wav, sr, duration)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=audio_segment[0], sr=sr, n_mfcc=128)\n",
    "\n",
    "    downsampled = downsample_with_max_pooling(mfcc, (1, 4))\n",
    "\n",
    "    return downsampled\n",
    "\n",
    "def extract_mfcc(src_file_path, sample_rate, utterance_duration):\n",
    "    # Extract MFCC features\n",
    "    mfcc = mfcc_from_audio_file(src_file_path, sample_rate, utterance_duration)\n",
    "\n",
    "    # Extract Pitch (Fundamental Frequency) and intensity\n",
    "    f0, intensity = pitch_and_intensity_from_audio(src_file_path, sample_rate, utterance_duration)\n",
    "    \n",
    "    # Combine MFCC, Delta, Delta-Delta, Pitch, and Intensity features\n",
    "    combined_features = np.concatenate((mfcc, f0, intensity), axis=0)\n",
    "    \n",
    "    # Get the emotion class of the audio\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086964cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define your class labels\n",
    "class_y = ['ang','hap','neu','sad']\n",
    "\n",
    "mfcc_features3 = extract_mfcc('../../iemocap/Ses05M_impro08_M019.wav',32750, 6)#neu-sad\n",
    "\n",
    "def prepare_input(mfcc_features):\n",
    "\n",
    "    mfcc_features = np.expand_dims(mfcc_features, axis=0)  # Add channel dimension\n",
    "    mfcc_features = np.expand_dims(mfcc_features, axis=0)\n",
    "    return torch.tensor(mfcc_features, dtype=torch.float32)\n",
    "\n",
    "mfcc_input3 = prepare_input(mfcc_features3) #neu\n",
    "mfcc_input3 = mfcc_input3.to(device)\n",
    "\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "# Perform inference\n",
    "cnn1_out, cnn2_out, cnn3_out, fl, lstm_out, l1_out, dr_out, l2_out, output = model(mfcc_input3)\n",
    "\n",
    "predicted_class = torch.argmax(output, dim=1).item()\n",
    "print(f\"Predicted Class: {class_y[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad08a5d-42ec-4c9f-9dff-f9cd0a642d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_features3 = np.expand_dims(mfcc_features3, axis=0)\n",
    "input_data = torch.tensor(mfcc_features3, requires_grad=True).float().to(device)\n",
    "baseline_data = torch.zeros_like(input_data)  # Baseline input (e.g., zero vector)\n",
    "\n",
    "# Compute Integrated Gradients\n",
    "attributions, error = integrated_gradients(model, input_data, baseline_data, target_class=3, steps=50)\n",
    "\n",
    "# Visualize the attributions\n",
    "feature_names = [f\"Feature {i+1}\" for i in range(96)]\n",
    "\n",
    "# Visualize with feature names\n",
    "visualize_integrated_gradients(attributions, feature_names=feature_names, aggregation='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f86c13-ee2f-41af-9c51-1ea745f5047a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_insertion_deletion_scores(\n",
    "    model, input_tensor, attributions, target_class, baseline=None, steps=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute Insertion and Deletion metrics for the Integrated Gradients attributions.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Trained model for prediction.\n",
    "        input_tensor: Original input for which attributions are computed.\n",
    "        attributions: Feature attributions (same shape as input_tensor).\n",
    "        target_class: Class index for which to evaluate.\n",
    "        baseline: Reference input (default is all zeros).\n",
    "        steps: Number of steps for gradual insertion/deletion.\n",
    "    \n",
    "    Returns:\n",
    "        insertion_scores: List of scores during feature insertion.\n",
    "        deletion_scores: List of scores during feature deletion.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define baseline (default: zero tensor of the same shape as input)\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(input_tensor)\n",
    "    \n",
    "    # Rank features by absolute attribution values (descending order)\n",
    "    ranked_indices = torch.argsort(-torch.abs(attributions.view(-1)))  # Flatten input\n",
    "    \n",
    "    # Insertion: Start with baseline and gradually add features\n",
    "    insertion_scores = []\n",
    "    insertion_input = baseline.clone()\n",
    "    for i in range(steps + 1):\n",
    "        k = int(i * len(ranked_indices) / steps)  # Compute the number of features to add\n",
    "        if k > 0:\n",
    "            indices_to_insert = ranked_indices[:k]\n",
    "            insertion_input.view(-1)[indices_to_insert] = input_tensor.view(-1)[indices_to_insert]\n",
    "        \n",
    "        # Compute model output for target class\n",
    "        output = model(insertion_input.unsqueeze(0))\n",
    "\n",
    "        # If the output is a tuple (for multi-output models), extract the first element\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "\n",
    "        \n",
    "        # If the output is 4D (e.g., from convolutional layers), flatten it\n",
    "        if len(output.shape) == 4:\n",
    "            # Apply global average pooling (or use flatten if needed)\n",
    "            output = output.view(output.size(0), -1)  # Flatten all dimensions except batch\n",
    "        \n",
    "        # If you need to reduce to a specific target class (classification), use:\n",
    "        if len(output.shape) == 2:\n",
    "            output = output[:, target_class]  # For classification, select the target class\n",
    "        \n",
    "        # If regression, use the output directly (e.g., if it's scalar)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.item()  # Convert the scalar output to a Python float\n",
    "        \n",
    "        insertion_scores.append(output)\n",
    "    \n",
    "    # Deletion: Start with the original input and gradually remove features\n",
    "    deletion_scores = []\n",
    "    deletion_input = input_tensor.clone()\n",
    "    for i in range(steps + 1):\n",
    "        k = int(i * len(ranked_indices) / steps)  # Compute the number of features to remove\n",
    "        if k > 0:\n",
    "            indices_to_delete = ranked_indices[:k]\n",
    "            deletion_input.view(-1)[indices_to_delete] = baseline.view(-1)[indices_to_delete]\n",
    "        \n",
    "        # Compute model output for target class\n",
    "        output = model(deletion_input.unsqueeze(0))\n",
    "\n",
    "        # Check if the output is a tuple (e.g., model has multiple outputs)\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]  # Extract the first element (the main output)\n",
    "        \n",
    "        # If the output is 4D (e.g., from convolutional layers), flatten it\n",
    "        if len(output.shape) == 4:\n",
    "            output = output.view(output.size(0), -1)  # Flatten all dimensions except batch\n",
    "        \n",
    "        # If you need to reduce to a specific target class (classification), use:\n",
    "        if len(output.shape) == 2:\n",
    "            output = output[:, target_class]  # For classification, select the target class\n",
    "        \n",
    "        # If regression, use the output directly (e.g., if it's scalar)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.item()  # Convert the scalar output to a Python float\n",
    "    \n",
    "\n",
    "        deletion_scores.append(output)\n",
    "    \n",
    "    return insertion_scores, deletion_scores\n",
    "\n",
    "\n",
    "# Example: Compute Insertion and Deletion Scores\n",
    "insertion_scores, deletion_scores = compute_insertion_deletion_scores(\n",
    "    model=model,\n",
    "    input_tensor=input_data,\n",
    "    attributions=attributions,\n",
    "    target_class=3,  # Adjust based on your target class\n",
    "    baseline=baseline_data,\n",
    "    steps=50\n",
    ")\n",
    "\n",
    "# Visualization of Insertion and Deletion Scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(len(insertion_scores)), insertion_scores, label=\"Insertion\", color=\"green\")\n",
    "plt.plot(range(len(deletion_scores)), deletion_scores, label=\"Deletion\", color=\"red\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Model Output\")\n",
    "plt.title(\"Insertion and Deletion Metrics\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"insertion_deletion_plot.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Insertion and Deletion plot saved as 'insertion_deletion_plot_ang.png'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
